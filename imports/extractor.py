from capstone import *
import pandas as pd
import pefile
import os

def start_extraction(root_folder_name, output_file):
    # Check if the root folder exists
    try:
        folders = os.listdir(root_folder_name)
    except FileNotFoundError:
        print('Root folder not found. Please check the path and try again.')
        return
    
    fail_counter = 0
    success_counter = 0
    features_dict = {}
    print('Starting extraction...')

    # Iterate through the folders in the root folder and extract features for each item
    for folder in folders:

        # Get the items in the folder
        folder_path = os.path.join(root_folder_name, folder)

        # Skip if the item is not a folder
        if not os.path.isdir(folder_path):
            continue

        # Get the items in the folder
        items = os.listdir(folder_path)

        # Skip if the folder is empty
        if not items:
            continue
        
        # Extract features for each item in the folder
        for item in items:
            item_url = os.path.join(folder_path, item)
            result = extract_features(item_url)
            
            if result is not None:
                features_dict[item] = result
                print('[SUCCESS]', item_url)
                success_counter += 1
            else:
                print('[FAILED]', item_url)
                fail_counter += 1

    # Print the extraction summary
    print(f'\nExtraction Completed. {success_counter} successfull, {fail_counter} failed. {round(success_counter / (success_counter + fail_counter) * 100, 2)}% success rate.')

    # Convert the dictionary to a DataFrame and save to a CSV file
    try:
        df = pd.DataFrame.from_dict(features_dict, orient='index')
        df.to_csv(output_file, index=False)
        print(f'Data saved to {output_file}')
        return df
    except Exception as e:
        print('An error occurred while saving the file:', e)


def extract_features(path):
    # Check if the file is a valid PE file
    try:
        sample = pefile.PE(path)
    except Exception as e:
        print('Error:', e)
        return None

    # Extract and features to the dictionary
    features = {}
    features['Name'] = path.split('\\')[-1]
    attributes = [
        'OPTIONAL_HEADER.CheckSum', 'FILE_HEADER.Machine', 'FILE_HEADER.SizeOfOptionalHeader',
        'FILE_HEADER.Characteristics', 'OPTIONAL_HEADER.MajorLinkerVersion', 'OPTIONAL_HEADER.MinorLinkerVersion',
        'OPTIONAL_HEADER.SizeOfCode', 'OPTIONAL_HEADER.SizeOfInitializedData', 'OPTIONAL_HEADER.SizeOfUninitializedData',
        'OPTIONAL_HEADER.AddressOfEntryPoint', 'OPTIONAL_HEADER.BaseOfCode', 'OPTIONAL_HEADER.BaseOfData',
        'OPTIONAL_HEADER.ImageBase', 'OPTIONAL_HEADER.SectionAlignment', 'OPTIONAL_HEADER.FileAlignment',
        'OPTIONAL_HEADER.MajorOperatingSystemVersion', 'OPTIONAL_HEADER.MinorOperatingSystemVersion',
        'OPTIONAL_HEADER.MajorImageVersion', 'OPTIONAL_HEADER.MinorImageVersion', 'OPTIONAL_HEADER.MajorSubsystemVersion',
        'OPTIONAL_HEADER.MinorSubsystemVersion', 'OPTIONAL_HEADER.SizeOfImage', 'OPTIONAL_HEADER.SizeOfHeaders',
        'OPTIONAL_HEADER.CheckSum', 'OPTIONAL_HEADER.Subsystem', 'OPTIONAL_HEADER.DllCharacteristics',
        'OPTIONAL_HEADER.SizeOfStackReserve', 'OPTIONAL_HEADER.SizeOfStackCommit', 'OPTIONAL_HEADER.SizeOfHeapReserve',
        'OPTIONAL_HEADER.SizeOfHeapCommit', 'OPTIONAL_HEADER.LoaderFlags', 'OPTIONAL_HEADER.NumberOfRvaAndSizes',
        'FILE_HEADER.NumberOfSections'
    ]

    # Add the features to the dictionary
    for attr in attributes:
        parts = attr.split('.')
        obj = sample
        for part in parts:
            if hasattr(obj, part):
                obj = getattr(obj, part)
            else:
                obj = None
                break
        if obj is not None:
            features[attr] = obj
        else:
            features[attr] = 0

    # Extract the entropy, raw size and virtual size of the sections
    MeanEntropy, MaxEntropy, MinEntropy = [], [], []
    MeanRawsize, MaxRawsize, MinRawsize = [], [], []
    MeanVirtualsize, MaxVirtualsize, MinVirtualsize = [], [], []

    for section in sample.sections:
        MeanEntropy.append(section.get_entropy())
        MeanRawsize.append(section.SizeOfRawData)
        MeanVirtualsize.append(section.Misc_VirtualSize)
        MaxEntropy.append(section.get_entropy())
        MaxRawsize.append(section.SizeOfRawData)
        MaxVirtualsize.append(section.Misc_VirtualSize)
        MinEntropy.append(section.get_entropy())
        MinRawsize.append(section.SizeOfRawData)
        MinVirtualsize.append(section.Misc_VirtualSize)

    features['MeanEntropy'] = sum(MeanEntropy) / float(len(MeanEntropy))
    features['MeanRawsize'] = sum(MeanRawsize) / float(len(MeanRawsize))
    features['MeanVirtualsize'] = sum(MeanVirtualsize) / float(len(MeanVirtualsize))
    features['MaxEntropy'] = max(MaxEntropy)
    features['MaxRawsize'] = max(MaxRawsize)
    features['MaxVirtualsize'] = max(MaxVirtualsize)
    features['MinEntropy'] = min(MinEntropy)
    features['MinRawsize'] = min(MinRawsize)
    features['MinVirtualsize'] = min(MinVirtualsize)

    # Extract the number of imports and the number of DLLs
    try:
        # Extract number of DLLs
        features['ImportsNbDLL'] = len(sample.DIRECTORY_ENTRY_IMPORT)
        
        # Extract number of imports from the DLLs
        import_count = 0
        for entry in sample.DIRECTORY_ENTRY_IMPORT:
            import_count += len(entry.imports)

        features['ImportsNb'] = import_count

        # extracting the imports names
        imports = []
        for entry in sample.DIRECTORY_ENTRY_IMPORT:
            for imp in entry.imports:
                imports.append(imp.name.decode('utf-8'))

        # Add the imports to the dictionary
        features['DLLImports'] = ' '.join(imports)
    except:
        features['ImportsNbDLL'] = 0
        features['ImportsNb'] = 0
        features['DLLImports'] = ''

    # features_dict['ExportNb'] = len(sample.DIRECTORY_ENTRY_EXPORT.symbols)
    # features_dict['ResourcesNb'] = len(sample.DIRECTORY_ENTRY_RESOURCE.entries)
    # features_dict['ResourcesMeanEntropy'] = sample.RESOURCES_MEAN_ENTROPY
    # features_dict['ResourcesMinEntropy'] = sample.RESOURCES_MIN_ENTROPY
    # features_dict['ResourcesMaxEntropy'] = sample.RESOURCES_MAX_ENTROPY
    # features_dict['ResourcesMeanSize'] = sample.RESOURCES_MEAN_SIZE
    # features_dict['ResourcesMinSize'] = sample.RESOURCES_MIN_SIZE
    # features_dict['ResourcesMaxSize'] = sample.RESOURCES_MAX_SIZE
    # features_dict['LoadConfigurationSize'] = sample.LOAD_CONFIGURATION_SIZE
    # features_dict['VersionInformationSize'] = sample.VERSION_INFORMATION_SIZE

    # opcodes extraction
    # try:
    #     opcodes = []
    #     for section in sample.sections:
    #         if section.Characteristics & 0x20:
    #             code = section.get_data()
    #             # decode the code and print the disassembled code
    #             disassembler = Cs(CS_ARCH_X86, CS_MODE_32)
    #             for instruction in disassembler.disasm(code, section.VirtualAddress):
    #                 opcodes.append(instruction.mnemonic)

    #     # Add the opcodes to the dictionary
    #     features['Opcodes'] = ' '.join(opcodes)
    # except:
    #     features['Opcodes'] = ''
        
    # Add the family name to the dictionary
    features['Family'] = path.split('\\')[-2]
    return features